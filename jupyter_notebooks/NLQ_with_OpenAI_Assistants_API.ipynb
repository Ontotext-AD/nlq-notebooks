{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9gXAYm94dRe"
   },
   "source": [
    "# NLQ with OpenAI Assistants API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jl1Rra_LpI7g"
   },
   "source": [
    "## Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmTuC0DNpIga"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger('')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMboG4AZ8m4e"
   },
   "source": [
    "## Create chat assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GraphDB\n",
    "\n",
    "You need a running GraphDB instance. This tutorial shows how to run the database locally using the GraphDB Docker image. It provides a docker compose set-up, which populates GraphDB with the Star Wars dataset.\n",
    "\n",
    "- Install [Docker](https://docs.docker.com/get-docker/). This tutorial is created using Docker version 28.0.1 which bundles Docker Compose. For earlier Docker versions you may need to install Docker Compose separately.\n",
    "- Start GraphDB with the following script executed from the `docker` folder\n",
    "\n",
    "```\n",
    "docker build --tag graphdb .\n",
    "docker compose up -d graphdb\n",
    "```\n",
    "\n",
    "You need to wait a couple of seconds for the database to start on http://localhost:7200/. The Star Wars dataset starwars-data.ttl is automatically loaded into the `starwars` repository. The local SPARQL endpoint http://localhost:7200/repositories/starwars can be used to run queries against. You can also open the GraphDB Workbench from your favourite web browser http://localhost:7200/sparql where you can make queries interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from ttyg.graphdb import GraphDB\n",
    "\n",
    "graphdb_base_url = \"http://localhost:7200\"\n",
    "graphdb_repository_id = \"starwars\"\n",
    "\n",
    "# If GraphDB is not secured\n",
    "graph = GraphDB(\n",
    "    base_url=graphdb_base_url,\n",
    "    repository_id=graphdb_repository_id,\n",
    ")\n",
    "\n",
    "# If GraphDB is secured, you can use the auth_header parameter to pass the value of the \"Authorization\" header.\n",
    "# The example below uses a basic authentication.\n",
    "# username, password = \"admin\", \"root\"\n",
    "# graph = GraphDB(\n",
    "#     base_url=graphdb_base_url,\n",
    "#     repository_id=graphdb_repository_id,\n",
    "#     auth_header=\"Basic \" + b64encode(f\"{username}:{password}\".encode(\"ascii\")).decode(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "llm_model = \"gpt-4o\"\n",
    "llm_temperature = 0\n",
    "\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "openai_client = openai.OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Azure OpenAI\n",
    "\n",
    "Alternatively, you can use Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "set_env(\"AZURE_OPENAI_API_KEY\")\n",
    "api_version = \"2024-05-01-preview\"\n",
    "llm_model = \"gpt-4o\"\n",
    "azure_endpoint = \"https://{workspace-id}.openai.azure.com/\"\n",
    "llm_temperature = 0\n",
    "\n",
    "openai_client = openai.AzureOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ttyg.tools import (\n",
    "    FTSTool,\n",
    "    IRIDiscoveryTool,\n",
    "    NowTool,\n",
    "    RetrievalQueryTool,\n",
    "    SimilaritySearchQueryTool,\n",
    "    SparqlQueryTool,\n",
    ")\n",
    "\n",
    "ontology_schema_file_path = Path(\"..\") / \"docker\" / \"SWAPI-ontology.ttl\"\n",
    "sparql_query_tool = SparqlQueryTool(\n",
    "    graph=graph,\n",
    "    ontology_schema_file_path=ontology_schema_file_path,\n",
    ")\n",
    "\n",
    "# The full-text search (FTS) must be enabled for the repository in order to use this tool.\n",
    "# For details how to enable it check the documentation https://graphdb.ontotext.com/documentation/10.8/full-text-search.html#simple-full-text-search-index .\n",
    "# It's also recommended to compute the RDF rank for the repository.\n",
    "# For details how to compute it refer to the documentation https://graphdb.ontotext.com/documentation/10.8/ranking-results.html .\n",
    "fts_tool = FTSTool(\n",
    "    graph=graph,\n",
    ")\n",
    "\n",
    "# The full-text search (FTS) must be enabled for the repository in order to use this tool.\n",
    "# For details how to enable it check the documentation https://graphdb.ontotext.com/documentation/10.8/full-text-search.html#simple-full-text-search-index .\n",
    "# It's also recommended to compute the RDF rank for the repository.\n",
    "# For details how to compute it refer to the documentation https://graphdb.ontotext.com/documentation/10.8/ranking-results.html .\n",
    "iri_discovery_tool = IRIDiscoveryTool(\n",
    "    graph=graph,\n",
    ")\n",
    "\n",
    "# ChatGPT Retrieval Plugin Connector must exist in order to use this tool.\n",
    "# In order to set up the ChatGPT Retrieval Connector Tool with an open source LLM, contact Graphwise, doing business as Ontotext, for additional help.\n",
    "# retrieval_connector_name = \"retrievalConnector\"\n",
    "# retrieval_query_tool = RetrievalQueryTool(\n",
    "#     graph=graph,\n",
    "#     connector_name=retrieval_connector_name,\n",
    "# )\n",
    "\n",
    "# Similarity Index must exist in order to use this tool.\n",
    "similarity_index_name = \"similarityIndex\"\n",
    "similarity_score_threshold = 0.9\n",
    "similarity_query_tool = SimilaritySearchQueryTool(\n",
    "    graph=graph,\n",
    "    index_name=similarity_index_name,\n",
    "    similarity_score_threshold=similarity_score_threshold,\n",
    ")\n",
    "\n",
    "now_tool = NowTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUThErZfbehc",
    "outputId": "3a2b25ba-75f8-4193-cbfd-cf3f1217ec17"
   },
   "outputs": [],
   "source": [
    "from ttyg.agents import OpenAIAssistant\n",
    "from ttyg.tools import Toolkit\n",
    "\n",
    "instructions = f\"\"\"You are a natural language querying assistant, and you answer the users' questions.\n",
    "If you need to write a SPARQL query, use only the classes and properties provided in the schema and don't invent or guess any.\n",
    "Include all the prefixes from the ontology schema in the SPARQL queries.\n",
    "The ontology schema in turtle format to use in SPARQL queries is:\n",
    "```turtle\n",
    "{sparql_query_tool.schema_graph.serialize(format='turtle')}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "assistant = OpenAIAssistant.create(\n",
    "    model=llm_model,\n",
    "    temperature=llm_temperature,\n",
    "    instructions=instructions,\n",
    "    openai_client=openai_client,\n",
    "    toolkit=Toolkit([\n",
    "        sparql_query_tool,\n",
    "        fts_tool,\n",
    "        iri_discovery_tool,\n",
    "        # retrieval_query_tool,\n",
    "        similarity_query_tool,\n",
    "        now_tool,\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEOw_aKX68cQ"
   },
   "source": [
    "## Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RaCbjmo5vLqs",
    "outputId": "5c373ebb-033f-431c-9ef3-ecbf42159850"
   },
   "outputs": [],
   "source": [
    "user_question = \"How many Star Wars movies are there\"\n",
    "\n",
    "thread_id = assistant.create_thread().id\n",
    "logging.debug(f\"Started new thread {thread_id}\")\n",
    "response = assistant.create_message_and_run(thread_id, user_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4vwO2EWfv6Ac",
    "outputId": "a2a24cf9-fab1-438a-d4bb-f96a1a94c3d7"
   },
   "outputs": [],
   "source": [
    "user_question = \"How many awards each of them received\"\n",
    "\n",
    "response = assistant.create_message_and_run(thread_id, user_question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NC88s1uHvMnj"
   },
   "source": [
    "## Delete the thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 460
    },
    "id": "JKjIkHx3vPM4",
    "outputId": "d53c35cd-76f3-49ed-8fb4-2c6b128b3014"
   },
   "outputs": [],
   "source": [
    "from openai.types.beta import ThreadDeleted\n",
    "\n",
    "thread_deleted: ThreadDeleted = assistant.delete_thread(thread_id)\n",
    "logging.info(f\"Thread {thread_deleted.id} is deleted ? {thread_deleted.deleted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QX39IH9UsIvk"
   },
   "source": [
    "## Delete the Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcjDbPmYsI7d",
    "outputId": "c5c3f5b6-1b7a-4249-bded-3ecf44f16d8c"
   },
   "outputs": [],
   "source": [
    "from openai.types.beta import AssistantDeleted\n",
    "\n",
    "assistant_deleted: AssistantDeleted = assistant.delete_assistant()\n",
    "logging.info(f\"Assistant {assistant_deleted.id} is deleted ? {assistant_deleted.deleted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're finished playing with NLQ with GraphDB, you can shut down the Docker environment by running \n",
    "```\n",
    "docker compose down -v --remove-orphans\n",
    "```\n",
    "from the `docker` directory."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
