{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d51186b-568a-4eae-9d2e-83a7084f920b",
   "metadata": {},
   "source": [
    "# NLQ with LangGraph ReAct Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45af3c3-acbd-4bab-aa98-df7f663ce7e1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbab90e-3eff-4547-96e0-6e42851f4539",
   "metadata": {},
   "source": [
    "### Run GraphDB\n",
    "\n",
    "You need a running GraphDB instance. This tutorial shows how to run the database locally using the GraphDB Docker image. It provides a docker compose set-up, which populates GraphDB with the Star Wars dataset.\n",
    "\n",
    "- Install [Docker](https://docs.docker.com/get-docker/). This tutorial is created using Docker version 28.0.1 which bundles Docker Compose. For earlier Docker versions you may need to install Docker Compose separately.\n",
    "- Start GraphDB with the following script executed from the `docker` folder\n",
    "\n",
    "```\n",
    "docker build --tag graphdb .\n",
    "docker compose up -d graphdb\n",
    "```\n",
    "\n",
    "You need to wait a couple of seconds for the database to start on http://localhost:7200/. The Star Wars dataset starwars-data.ttl is automatically loaded into the `starwars` repository. The local SPARQL endpoint http://localhost:7200/repositories/starwars can be used to run queries against. You can also open the GraphDB Workbench from your favourite web browser http://localhost:7200/sparql where you can make queries interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56682ae-762f-49c6-8e4c-426f6f1a76b1",
   "metadata": {},
   "source": [
    "### GraphDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbd7af-c85d-4e46-8235-2864cc250c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from ttyg.graphdb import GraphDB\n",
    "\n",
    "graphdb_base_url = \"http://localhost:7200\"\n",
    "graphdb_repository_id = \"starwars\"\n",
    "\n",
    "# If GraphDB is not secured\n",
    "graph = GraphDB(\n",
    "    base_url=graphdb_base_url,\n",
    "    repository_id=graphdb_repository_id,\n",
    ")\n",
    "\n",
    "# If GraphDB is secured, you can use the auth_header parameter to pass the value of the \"Authorization\" header.\n",
    "# The example below uses a basic authentication.\n",
    "# username, password = \"admin\", \"root\"\n",
    "# graph = GraphDB(\n",
    "#     base_url=graphdb_base_url,\n",
    "#     repository_id=graphdb_repository_id,\n",
    "#     auth_header=\"Basic \" + b64encode(f\"{username}:{password}\".encode(\"ascii\")).decode(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf48eb7-fd2f-410d-a8ef-39cb096d30a0",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17c6db-3060-42ee-8f4d-f1f2cf79af65",
   "metadata": {},
   "source": [
    "#### ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fdc22-a247-49f9-871a-8614eaf6af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "gpt_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-2024-05-13\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d5ef4-2ec7-434d-8458-3252af305e43",
   "metadata": {},
   "source": [
    "#### Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10690b-5051-44b1-b47f-fe8c3ecc10b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "# See [Authentication Documentation](https://docs.databricks.com/en/dev-tools/auth/index.html#databricks-personal-access-tokens) for how to get an access token\n",
    "set_env(\"DATABRICKS_TOKEN\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"https://{workspace-id}.cloud.databricks.com\"\n",
    "\n",
    "llama_model = ChatDatabricks(\n",
    "    model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ccc15-ff4c-4996-aea6-73f9cca7b303",
   "metadata": {},
   "source": [
    "#### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e398c-e593-45a3-b00b-875b6c03bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "set_env(\"ANTHROPIC_API_KEY\")\n",
    "claude_model = ChatAnthropic(\n",
    "    model_name=\"claude-3-haiku-20240307\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970f0cb-e50b-4c9f-b1f3-7cc975d969c9",
   "metadata": {},
   "source": [
    "## Define the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01517e71-62b0-46b1-8c01-7c985ce55d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ttyg.tools import (\n",
    "    FTSTool,\n",
    "    IRIDiscoveryTool,\n",
    "    NowTool,\n",
    "    RetrievalQueryTool,\n",
    "    SimilaritySearchQueryTool,\n",
    "    SparqlQueryTool,\n",
    ")\n",
    "\n",
    "ontology_schema_file_path = Path(\"..\") / \"docker\" / \"SWAPI-ontology.ttl\"\n",
    "sparql_query_tool = SparqlQueryTool(\n",
    "    graph=graph,\n",
    "    ontology_schema_file_path=ontology_schema_file_path,\n",
    ")\n",
    "\n",
    "# The full-text search (FTS) must be enabled for the repository in order to use this tool.\n",
    "# For details how to enable it check the documentation https://graphdb.ontotext.com/documentation/10.8/full-text-search.html#simple-full-text-search-index .\n",
    "# It's also recommended to compute the RDF rank for the repository.\n",
    "# For details how to compute it refer to the documentation https://graphdb.ontotext.com/documentation/10.8/ranking-results.html .\n",
    "fts_tool = FTSTool(\n",
    "    graph=graph,\n",
    ")\n",
    "\n",
    "# The full-text search (FTS) must be enabled for the repository in order to use this tool.\n",
    "# For details how to enable it check the documentation https://graphdb.ontotext.com/documentation/10.8/full-text-search.html#simple-full-text-search-index .\n",
    "# It's also recommended to compute the RDF rank for the repository.\n",
    "# For details how to compute it refer to the documentation https://graphdb.ontotext.com/documentation/10.8/ranking-results.html .\n",
    "iri_discovery_tool = IRIDiscoveryTool(\n",
    "    graph=graph,\n",
    ")\n",
    "\n",
    "# ChatGPT Retrieval Plugin Connector must exist in order to use this tool.\n",
    "# In order to set up the ChatGPT Retrieval Connector Tool with an open source LLM, contact Graphwise, doing business as Ontotext, for additional help.\n",
    "# retrieval_connector_name = \"retrievalConnector\"\n",
    "# retrieval_query_tool = RetrievalQueryTool(\n",
    "#     graph=graph,\n",
    "#     connector_name=retrieval_connector_name,\n",
    "# )\n",
    "\n",
    "# Similarity Index must exist in order to use this tool.\n",
    "similarity_index_name = \"similarityIndex\"\n",
    "similarity_score_threshold = 0.9\n",
    "similarity_query_tool = SimilaritySearchQueryTool(\n",
    "    graph=graph,\n",
    "    index_name=similarity_index_name,\n",
    "    similarity_score_threshold=similarity_score_threshold,\n",
    ")\n",
    "\n",
    "now_tool = NowTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d1b73-8db2-4c39-bb13-27e6b899e8eb",
   "metadata": {},
   "source": [
    "## Create the ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1402b7-118c-445e-b670-62cd3df1b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "instructions = f\"\"\"You are a natural language querying assistant, and you answer the users' questions.\n",
    "If you need to write a SPARQL query, use only the classes and properties provided in the schema and don't invent or guess any.\n",
    "Include all the prefixes from the ontology schema in the SPARQL queries.\n",
    "The ontology schema in turtle format to use in SPARQL queries is:\n",
    "```turtle\n",
    "{sparql_query_tool.schema_graph.serialize(format='turtle')}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "agent_executor = create_react_agent(\n",
    "    model=gpt_model,\n",
    "    tools=[\n",
    "        sparql_query_tool,\n",
    "        fts_tool,\n",
    "        iri_discovery_tool,\n",
    "        # retrieval_query_tool,\n",
    "        similarity_query_tool,\n",
    "        now_tool,\n",
    "    ],\n",
    "    state_modifier=instructions,\n",
    "    checkpointer=MemorySaver(),\n",
    "    # debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d75fa-ed06-424a-8a51-cedcba5096f3",
   "metadata": {},
   "source": [
    "## Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53437127-7989-4692-9ceb-fe9542b17d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(agent, inputs, config):\n",
    "    for s in agent.stream(inputs, config, stream_mode=\"values\"):\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "\n",
    "conf = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "messages = {\"messages\": [(\"user\", \"How many Star Wars movies are there\")]}\n",
    "print_stream(agent_executor, messages, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8cc36-08b2-4e2c-a520-f6e16a1da4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\"messages\": [(\"user\", \"How many awards each of them received\")]}\n",
    "print_stream(agent_executor, messages, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a119269-1338-4bc9-a3bc-5e42e44ba965",
   "metadata": {},
   "source": [
    "## Create ReAct agent with user confirmation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfdbd7-c178-4e83-86a3-ac9d0d173ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(\n",
    "    model=gpt_model,\n",
    "    tools=[\n",
    "        sparql_query_tool,\n",
    "        fts_tool,\n",
    "        iri_discovery_tool,\n",
    "        # retrieval_query_tool,\n",
    "        similarity_query_tool,\n",
    "        now_tool,\n",
    "    ],\n",
    "    interrupt_before=[\"tools\"],\n",
    "    state_modifier=instructions,\n",
    "    checkpointer=MemorySaver(),\n",
    "    # debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4412a-be45-4ef6-b021-99f220cc4f44",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc457f-79dd-4516-a539-dafdc6922bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\"configurable\": {\"thread_id\": \"thread-2\"}}\n",
    "messages = {\"messages\": [(\"user\", \"How many movie characters are in the dataset\")]}\n",
    "print_stream(agent_executor, messages, conf)\n",
    "snapshot = agent_executor.get_state(conf)\n",
    "print(\"Next step: \", snapshot.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a15ea-1cee-4f39-a1a3-0483251691f2",
   "metadata": {},
   "source": [
    "### Ask for user confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12be27-1df8-4b73-b968-21e6463c5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "proceed = input(\"Do you want to proceed? (yes/no): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f065ce-fe53-4653-98b1-063d402bc9ae",
   "metadata": {},
   "source": [
    "### Optionally proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456682d-bb57-440f-b647-5d6563936292",
   "metadata": {},
   "outputs": [],
   "source": [
    "if proceed == \"yes\":\n",
    "    print_stream(agent_executor, None, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113559cb-7197-49d5-ae71-2ac40550594b",
   "metadata": {},
   "source": [
    "When you're finished playing with NLQ with GraphDB, you can shut down the Docker environment by running \n",
    "```\n",
    "docker compose down -v --remove-orphans\n",
    "```\n",
    "from the `docker` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
