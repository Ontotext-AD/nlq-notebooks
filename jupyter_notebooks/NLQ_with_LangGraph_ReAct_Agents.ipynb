{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d51186b-568a-4eae-9d2e-83a7084f920b",
   "metadata": {},
   "source": [
    "# NLQ with LangGraph ReAct Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45af3c3-acbd-4bab-aa98-df7f663ce7e1",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af5e4c-056e-4e2b-bd22-7e3740d9a171",
   "metadata": {},
   "source": [
    "### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfca35e-add5-46ba-9d38-db4fd9990e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger('')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n",
    "\n",
    "logger.handlers.clear()\n",
    "logger.addHandler(handler)\n",
    "\n",
    "logging.getLogger(\"openai\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"httpcore\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbab90e-3eff-4547-96e0-6e42851f4539",
   "metadata": {},
   "source": [
    "### Run GraphDB\n",
    "\n",
    "You need a running GraphDB instance. You can start GraphDB with the following script executed from the `docker` folder\n",
    "\n",
    "```\n",
    "./start.sh\n",
    "```\n",
    "\n",
    "The database will start on `http://localhost:7200/`. The Star Wars dataset `starwars-data.ttl` is automatically loaded into the `starwars` repository. The local SPARQL endpoint `http://localhost:7200/repositories/starwars` can be used to run queries against. You can also open the GraphDB Workbench from your favourite web browser `http://localhost:7200/sparql` where you can make queries interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56682ae-762f-49c6-8e4c-426f6f1a76b1",
   "metadata": {},
   "source": [
    "### GraphDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbbd7af-c85d-4e46-8235-2864cc250c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from ttyg.graphdb import GraphDB\n",
    "\n",
    "graphdb_base_url = \"http://localhost:7200\"\n",
    "graphdb_repository_id = \"starwars\"\n",
    "\n",
    "# If GraphDB is not secured\n",
    "graph = GraphDB(\n",
    "    base_url=graphdb_base_url,\n",
    "    repository_id=graphdb_repository_id,\n",
    ")\n",
    "\n",
    "# If GraphDB is secured, you can use the auth_header parameter to pass the value of the \"Authorization\" header.\n",
    "# The example below uses a basic authentication.\n",
    "# username, password = \"admin\", \"root\"\n",
    "# graph = GraphDB(\n",
    "#     base_url=graphdb_base_url,\n",
    "#     repository_id=graphdb_repository_id,\n",
    "#     auth_header=\"Basic \" + b64encode(f\"{username}:{password}\".encode(\"ascii\")).decode(),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf48eb7-fd2f-410d-a8ef-39cb096d30a0",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17c6db-3060-42ee-8f4d-f1f2cf79af65",
   "metadata": {},
   "source": [
    "#### ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fdc22-a247-49f9-871a-8614eaf6af8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "set_env(\"OPENAI_API_KEY\")\n",
    "gpt_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-2024-05-13\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d5ef4-2ec7-434d-8458-3252af305e43",
   "metadata": {},
   "source": [
    "#### Databricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10690b-5051-44b1-b47f-fe8c3ecc10b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from databricks_langchain import ChatDatabricks\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "# See [Authentication Documentation](https://docs.databricks.com/en/dev-tools/auth/index.html#databricks-personal-access-tokens) for how to get an access token\n",
    "set_env(\"DATABRICKS_TOKEN\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = \"https://{workspace-id}.cloud.databricks.com\"\n",
    "\n",
    "llama_model = ChatDatabricks(\n",
    "    model=\"databricks-meta-llama-3-1-70b-instruct\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ccc15-ff4c-4996-aea6-73f9cca7b303",
   "metadata": {},
   "source": [
    "#### Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279e398c-e593-45a3-b00b-875b6c03bad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "from ttyg.utils import set_env\n",
    "\n",
    "set_env(\"ANTHROPIC_API_KEY\")\n",
    "claude_model = ChatAnthropic(\n",
    "    model_name=\"claude-3-haiku-20240307\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8970f0cb-e50b-4c9f-b1f3-7cc975d969c9",
   "metadata": {},
   "source": [
    "## Define the tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01517e71-62b0-46b1-8c01-7c985ce55d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ttyg.tools import (\n",
    "    FTSTool,\n",
    "    IRIDiscoveryTool,\n",
    "    NowTool,\n",
    "    RetrievalQueryTool,\n",
    "    SimilaritySearchQueryTool,\n",
    "    SparqlQueryTool,\n",
    ")\n",
    "\n",
    "ontology_schema_file_path = Path(\"..\") / \"docker\" / \"SWAPI-ontology.ttl\"\n",
    "sparql_query_tool = SparqlQueryTool(\n",
    "    graph=graph,\n",
    "    ontology_schema_file_path=ontology_schema_file_path,\n",
    ")\n",
    "\n",
    "# The full-text search (FTS) must be enabled for the repository in order to use this tool.\n",
    "# For details how to enable it check the documentation https://graphdb.ontotext.com/documentation/10.8/full-text-search.html#simple-full-text-search-index .\n",
    "# It's also recommended to compute the RDF rank for the repository.\n",
    "# For details how to compute it refer to the documentation https://graphdb.ontotext.com/documentation/10.8/ranking-results.html .\n",
    "fts_tool = FTSTool(\n",
    "    graph=graph,\n",
    ")\n",
    "\n",
    "# The full-text search (FTS) must be enabled for the repository in order to use this tool.\n",
    "# For details how to enable it check the documentation https://graphdb.ontotext.com/documentation/10.8/full-text-search.html#simple-full-text-search-index .\n",
    "# It's also recommended to compute the RDF rank for the repository.\n",
    "# For details how to compute it refer to the documentation https://graphdb.ontotext.com/documentation/10.8/ranking-results.html .\n",
    "iri_discovery_tool = IRIDiscoveryTool(\n",
    "    graph=graph,\n",
    ")\n",
    "\n",
    "# ChatGPT Retrieval Plugin Connector must exist in order to use this tool.\n",
    "# In order to set up the ChatGPT Retrieval Connector Tool with an open source LLM, contact Graphwise, doing business as Ontotext, for additional help.\n",
    "# retrieval_connector_name = \"retrievalConnector\"\n",
    "# retrieval_query_tool = RetrievalQueryTool(\n",
    "#     graph=graph,\n",
    "#     connector_name=retrieval_connector_name,\n",
    "# )\n",
    "\n",
    "# Similarity Index must exist in order to use this tool.\n",
    "similarity_index_name = \"similarityIndex\"\n",
    "similarity_score_threshold = 0.9\n",
    "similarity_query_tool = SimilaritySearchQueryTool(\n",
    "    graph=graph,\n",
    "    index_name=similarity_index_name,\n",
    "    similarity_score_threshold=similarity_score_threshold,\n",
    ")\n",
    "\n",
    "now_tool = NowTool()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07d1b73-8db2-4c39-bb13-27e6b899e8eb",
   "metadata": {},
   "source": [
    "## Create the ReAct agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1402b7-118c-445e-b670-62cd3df1b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "instructions = f\"\"\"You are a natural language querying assistant, and you answer the users' questions.\n",
    "If you need to write a SPARQL query, use only the classes and properties provided in the schema and don't invent or guess any.\n",
    "Include all the prefixes from the ontology schema in the SPARQL queries.\n",
    "The ontology schema in turtle format to use in SPARQL queries is:\n",
    "```turtle\n",
    "{sparql_query_tool.schema_graph.serialize(format='turtle')}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "agent_executor = create_react_agent(\n",
    "    model=gpt_model,\n",
    "    tools=[\n",
    "        sparql_query_tool,\n",
    "        fts_tool,\n",
    "        iri_discovery_tool,\n",
    "        # retrieval_query_tool,\n",
    "        similarity_query_tool,\n",
    "        now_tool,\n",
    "    ],\n",
    "    state_modifier=instructions,\n",
    "    checkpointer=MemorySaver(),\n",
    "    # debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d75fa-ed06-424a-8a51-cedcba5096f3",
   "metadata": {},
   "source": [
    "## Conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f175f8-72a9-47af-a45b-b9a1274150fd",
   "metadata": {},
   "source": [
    "Note, that at the moment the conversations history is not persisted and is kept in the memory. Upon shut down of the notebook, it will be lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7cfae-688b-4e93-9559-1df404ad6235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def print_stream(agent, inputs, config, last_message_id: str = None) -> str:\n",
    "    sum_input_tokens, sum_output_tokens, sum_total_tokens = 0, 0, 0\n",
    "\n",
    "    start = time.time()\n",
    "    for s in agent.stream(inputs, config, stream_mode=\"values\"):\n",
    "        messages = s[\"messages\"]\n",
    "        for message in reversed(messages):\n",
    "            if message.id == last_message_id:\n",
    "                break\n",
    "\n",
    "            message.pretty_print()\n",
    "            if hasattr(message, \"usage_metadata\"):\n",
    "                usage_metadata = message.usage_metadata\n",
    "                input_tokens, output_tokens, total_tokens = usage_metadata[\"input_tokens\"], usage_metadata[\"output_tokens\"], usage_metadata[\"total_tokens\"]\n",
    "                sum_input_tokens += input_tokens\n",
    "                sum_output_tokens += output_tokens\n",
    "                sum_total_tokens += total_tokens\n",
    "                logging.debug(\n",
    "                    f\"Usage: input tokens: {input_tokens}, \"\n",
    "                    f\"output tokens: {output_tokens}, \"\n",
    "                    f\"total tokens: {total_tokens}\")\n",
    "\n",
    "        last_message_id = messages[-1].id\n",
    "\n",
    "    logging.debug(\n",
    "        f\"Total usage: input tokens: {sum_input_tokens}, \"\n",
    "        f\"output tokens: {sum_output_tokens}, \"\n",
    "        f\"total tokens: {sum_total_tokens}\"\n",
    "    )\n",
    "    logging.debug(\n",
    "        f\"Elapsed time: {time.time() - start:.2f} seconds\"\n",
    "    )\n",
    "    return last_message_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53437127-7989-4692-9ceb-fe9542b17d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\"configurable\": {\"thread_id\": \"thread-1\"}}\n",
    "messages = {\"messages\": [(\"user\", \"How many Star Wars movies are there\")]}\n",
    "last_message_id = print_stream(agent_executor, messages, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8cc36-08b2-4e2c-a520-f6e16a1da4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = {\"messages\": [(\"user\", \"How many awards each of them received\")]}\n",
    "last_message_id = print_stream(agent_executor, messages, conf, last_message_id=last_message_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a119269-1338-4bc9-a3bc-5e42e44ba965",
   "metadata": {},
   "source": [
    "## Create ReAct agent with user confirmation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebfdbd7-c178-4e83-86a3-ac9d0d173ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = create_react_agent(\n",
    "    model=gpt_model,\n",
    "    tools=[\n",
    "        sparql_query_tool,\n",
    "        fts_tool,\n",
    "        iri_discovery_tool,\n",
    "        # retrieval_query_tool,\n",
    "        similarity_query_tool,\n",
    "        now_tool,\n",
    "    ],\n",
    "    interrupt_before=[\"tools\"],\n",
    "    state_modifier=instructions,\n",
    "    checkpointer=MemorySaver(),\n",
    "    # debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4412a-be45-4ef6-b021-99f220cc4f44",
   "metadata": {},
   "source": [
    "### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc457f-79dd-4516-a539-dafdc6922bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\"configurable\": {\"thread_id\": \"thread-2\"}}\n",
    "messages = {\"messages\": [(\"user\", \"How many movie characters are in the dataset\")]}\n",
    "last_message_id = print_stream(agent_executor, messages, conf)\n",
    "snapshot = agent_executor.get_state(conf)\n",
    "print(\"Next step: \", snapshot.next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576a15ea-1cee-4f39-a1a3-0483251691f2",
   "metadata": {},
   "source": [
    "### Ask for user confirmation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca12be27-1df8-4b73-b968-21e6463c5b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "proceed = input(\"Do you want to proceed? (yes/no): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f065ce-fe53-4653-98b1-063d402bc9ae",
   "metadata": {},
   "source": [
    "### Optionally proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9456682d-bb57-440f-b647-5d6563936292",
   "metadata": {},
   "outputs": [],
   "source": [
    "if proceed == \"yes\":\n",
    "    print_stream(agent_executor, None, conf, last_message_id=last_message_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113559cb-7197-49d5-ae71-2ac40550594b",
   "metadata": {},
   "source": [
    "When you're finished playing with NLQ with GraphDB, you can shut down the Docker environment by running \n",
    "```\n",
    "docker compose down -v --remove-orphans\n",
    "```\n",
    "from the `docker` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
